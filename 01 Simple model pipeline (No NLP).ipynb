{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import math\n",
    "from time import time\n",
    "\n",
    "# Pandas and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "# SK Learn requirements\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_path = os.getcwd() + \"\\\\data\\\\train.tsv\"\n",
    "train = pd.read_csv(train_path, sep='\\t')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "pd.np.random.seed(369)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Parse out individual category levels\n",
    "Product category names consist of subcategory trees (Men/Tops/T-shirts). There don't appear to be more than 5 levels and the vast majority of them have just 3 levels. Below, we create a variable for each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>category4</th>\n",
       "      <th>category5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Men</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-shirts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razer</td>\n",
       "      <td>0</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Tablets</td>\n",
       "      <td>Components &amp; Parts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Target</td>\n",
       "      <td>1</td>\n",
       "      <td>Women</td>\n",
       "      <td>Tops &amp; Blouses</td>\n",
       "      <td>Blouse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Home</td>\n",
       "      <td>Home Décor</td>\n",
       "      <td>Home Décor Accents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Women</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Necklaces</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand_name  shipping    category1            category2           category3  \\\n",
       "0        NaN         1          Men                 Tops            T-shirts   \n",
       "1      Razer         0  Electronics  Computers & Tablets  Components & Parts   \n",
       "2     Target         1        Women       Tops & Blouses              Blouse   \n",
       "3        NaN         1         Home           Home Décor  Home Décor Accents   \n",
       "4        NaN         0        Women              Jewelry           Necklaces   \n",
       "\n",
       "  category4 category5  \n",
       "0       NaN       NaN  \n",
       "1       NaN       NaN  \n",
       "2       NaN       NaN  \n",
       "3       NaN       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "train_x = train[[\"brand_name\", \"shipping\", \"category_name\"]]\n",
    "train_y = train.price\n",
    "\n",
    "def piece(string, delim, n):\n",
    "    string = str(string)\n",
    "    if string.count(delim) < n:\n",
    "        return pd.np.NaN\n",
    "    return string.split(delim)[n]\n",
    "\n",
    "train_x[\"category1\"] = train_x.category_name.map(lambda x: piece(x, \"/\", 0))\n",
    "train_x[\"category2\"] = train_x.category_name.map(lambda x: piece(x, \"/\", 1))\n",
    "train_x[\"category3\"] = train_x.category_name.map(lambda x: piece(x, \"/\", 2))\n",
    "train_x[\"category4\"] = train_x.category_name.map(lambda x: piece(x, \"/\", 3))\n",
    "train_x[\"category5\"] = train_x.category_name.map(lambda x: piece(x, \"/\", 4))\n",
    "train_x = train_x.drop(columns='category_name')\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill in missing data\n",
    "\n",
    "For now we will be representing any missing data with the string 'zMissing.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>category4</th>\n",
       "      <th>category5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zMissing</td>\n",
       "      <td>1</td>\n",
       "      <td>Men</td>\n",
       "      <td>Tops</td>\n",
       "      <td>T-shirts</td>\n",
       "      <td>zMissing</td>\n",
       "      <td>zMissing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Razer</td>\n",
       "      <td>0</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Computers &amp; Tablets</td>\n",
       "      <td>Components &amp; Parts</td>\n",
       "      <td>zMissing</td>\n",
       "      <td>zMissing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Target</td>\n",
       "      <td>1</td>\n",
       "      <td>Women</td>\n",
       "      <td>Tops &amp; Blouses</td>\n",
       "      <td>Blouse</td>\n",
       "      <td>zMissing</td>\n",
       "      <td>zMissing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zMissing</td>\n",
       "      <td>1</td>\n",
       "      <td>Home</td>\n",
       "      <td>Home Décor</td>\n",
       "      <td>Home Décor Accents</td>\n",
       "      <td>zMissing</td>\n",
       "      <td>zMissing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zMissing</td>\n",
       "      <td>0</td>\n",
       "      <td>Women</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Necklaces</td>\n",
       "      <td>zMissing</td>\n",
       "      <td>zMissing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand_name  shipping    category1            category2           category3  \\\n",
       "0   zMissing         1          Men                 Tops            T-shirts   \n",
       "1      Razer         0  Electronics  Computers & Tablets  Components & Parts   \n",
       "2     Target         1        Women       Tops & Blouses              Blouse   \n",
       "3   zMissing         1         Home           Home Décor  Home Décor Accents   \n",
       "4   zMissing         0        Women              Jewelry           Necklaces   \n",
       "\n",
       "  category4 category5  \n",
       "0  zMissing  zMissing  \n",
       "1  zMissing  zMissing  \n",
       "2  zMissing  zMissing  \n",
       "3  zMissing  zMissing  \n",
       "4  zMissing  zMissing  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.fillna('zMissing')\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encode\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features\n",
    "\n",
    "Create dummy variables out of categorical variables. First we use LabelEncoder to swap out category names with numbers. Then we use OneHotEncoder to pivot out those values in to columns.\n",
    "\n",
    "**OneHotEncoder**\n",
    "\n",
    "* Encode categorical integer features using a one-hot aka one-of-K scheme.\n",
    "* The input to this transformer should be a matrix of integers, denoting the values taken on by categorical (discrete) features.\n",
    "* The output will be a sparse matrix where each column corresponds to one possible value of one feature.\n",
    "* It is assumed that input features take on values in the range [0, n_values).\n",
    "* This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is proving to be too slow\n",
    "# dummies = pd.get_dummies(train_x)\n",
    "# lets try it with sklearn\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data_label_encoded = train_x.apply(label_encoder.fit_transform)\n",
    "train_x_encoded = encoder.fit_transform(data_label_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "LightGBM appears to be the defacto boosting algorithm right now. https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This competition uses root mean squared log error.\n",
    "# Lets use this to evaluate our models as well.\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# vectorized error calc\n",
    "def rmsle(y, y0):\n",
    "    assert len(y) == len(y0)\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))\n",
    "\n",
    "rmsle_score = make_scorer(rmsle, greater_is_better=False)\n",
    "\n",
    "def plot_scores(scores):\n",
    "    absolute = np.vectorize(abs)\n",
    "    scores = absolute(scores)\n",
    "    mean_score = round(scores.mean(), 5)\n",
    "    p = ggplot(pd.DataFrame({'scores': scores}), aes('scores')) + \\\n",
    "        geom_density(fill=\"lightblue\", alpha=1/3) + geom_point(aes(x=\"scores\", y=0), shape=6, size=4, colour=\"orange\") + \\\n",
    "        ggtitle(\"Mean 10-FCV RMSLE on Training Set: {0}\".format(mean_score)) + \\\n",
    "        ylab(\"Density\") + xlab(\"RMSLE\") + \\\n",
    "        theme_light()\n",
    "    return p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large … \n",
    "\n",
    "To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. \n",
    "With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features … \n",
    "\n",
    "Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.\n",
    "\n",
    "### Parameter Tuning\n",
    "#### learning_rate \n",
    "\n",
    "* Description: Each boosting iteration is supposed to provide an improvement to the training loss. The improvement is multiplied with the learning rate in order to perform smaller updates. Smaller updates allow to overfit slower the data, but requires more iterations for training. \n",
    "* Range: ]0, 8[\n",
    "* Major Impact: Model Performance, Number of Iterations \n",
    "* Minor Impact: Training Time \n",
    "* Strategy: \n",
    "  * Smaller is usually better when training, but set this larger for hyperparameter tuning. Consider using a learning rate of 0.05 or lower for training, while a learning rate of 0.10 or larger is used for tinkering the other hyperparameters. \n",
    "  * Once your learning rate is fixed, do not change it. It is not a good practice to consider the learning rate as a hyperparameter to tune. Learning rate should be set according to your training speed and performance tradeoff. Do not let an optimizer tune it. \n",
    "  * This hyperparameter needs to work well with num_iterations. For instance, doing 5 iteations at a learning rate of 0.1 approximately would require doing 5000 iterations at a learning rate of 0.001, which might be obnoxious for large datasets.\n",
    "\n",
    "#### num_iterations \n",
    "* Description: Number of boosting iterations. \n",
    "* Range: [1, 8[\n",
    "* Major Impact: Model Performance, Training Time \n",
    "* Minor Impact: RAM Usage \n",
    "* Strategy:\n",
    "  * Larger is not always better. Keep an eye on overfitting. Larger is usually better, until training data is overfitted too much. \n",
    "  * Typical: 110% of the mean of number of iterations from cross-validation, or use a very large number of iterations if using early stopping. \n",
    "  * Combine with early stopping to stop automatically boosting. \n",
    "\n",
    "#### early_stopping_round \n",
    "* Description: Number of maximum iterations without improvements. \n",
    "* Range: [0, 8[ \n",
    "* Major Impact: Model Performance, Number of Iterations, Training Time \n",
    "* Strategy:\n",
    "  * Larger is usually better. Typically around 50. \n",
    "  * Setting early stopping too large risks overfitting by not allowing training to stop due to luck. Scale this parameter appropriately with the learning rate (usually: linearly). \n",
    "  * Tips: make sure you added a validation dataset to watch, otherwise this parameter is useless.   \n",
    "  \n",
    "#### num_leaves (Use this in favor of maximum depth when using LightGBM) \n",
    "* Range: [1, ∞[ \n",
    "* Major Impact: Maximum Depth, Model Performance, Number of Iterations, Training Time, RAM Usage \n",
    "* Minor Impact: NULL \n",
    "* Description: Maximum leaves for each trained tree. Restricting the number of leaves acts as a regularization in order to not grow very deep trees.\n",
    "* Strategy: \n",
    "  * Larger is usually better, but overfitting speed increases. \n",
    "  * Typical: 255, usually {15, 31, 63, 127, 255, 511, 1023, 2047, 4095}. \n",
    "  * This is the most sensible hyperparameter for gradient boosting: tune it with the maximum depth. \n",
    "\n",
    "#### bagging_fraction \n",
    "* Description: Percentage of rows used per iteration frequency. Turns normal gradient descent in to stochastic gradient descent, which may not always be better.\n",
    "* Range: ]0, 1] \n",
    "* Major Impact: Early Stopping, Model Performance, Number of Iterations \n",
    "* Minor Impact: Training Time \n",
    "* Strategy:\n",
    "  * Try [0.70, .85, 1]\n",
    "  * In addition, this is the second most sensible hyperparameter for gradient boosting: tune it with the column sampling.\n",
    "\t\t\n",
    "#### feature_fraction\n",
    "\t• Range: ]0, 1]\n",
    "\t• Major Impact: Early Stopping, Model Performance, Number of Iterations\n",
    "\t• Minor Impact: Training Time\n",
    "\t• Description: Each model trained at each iteration will have only a specific % subset of features requested using subsample.\n",
    "\t• Strategy\n",
    "\t\t○ Smaller is usually better. Typically around 0.70.\n",
    "\t\t○ This is the second most sensible hyperparameter for gradient boosting: tune it with the row sampling.\n",
    "\n",
    "#### Boosting\n",
    "Description: Boosting method.\n",
    "  * \"gbdt\" (Gradient Boosted Decision Trees) which is the default boosting method using Decision Trees and Stochastic Gradient Descent;\n",
    "  * \"dart\" (Dropout Additive Regression Trees) is similar to Dropout in neural networks, except you are applying this idea to trees (dropping trees randomly).\n",
    "  * \"goss\" (Gradient-based One-Side Sampling) which is a method using subsampling to converge faster/better using Stochastic Gradient Descent. From the paper: With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size.\n",
    "* Major Impact: Model Performance, Number of Iterations, Training Time\n",
    "* Minor Impact: RAM Usage\n",
    "\n",
    "#### metric\n",
    "* If you want to focus on the mean, use 'mse.' If you want to focus on the median use 'mae.' See this post for more details.\n",
    "* huber: Less sensitive to outliers\n",
    "* fair: Supposedly a better way to measure mase\n",
    "* Poisson: Effective for count data.\n",
    "* Quantile\n",
    "* Quantile_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
